import pytorch_lightning as pl
import torch
import hydra
from omegaconf import DictConfig
import os

class BaseAudioSystem(pl.LightningModule):
    def __init__(self, cfg: DictConfig):
        super().__init__()
        self.save_hyperparameters()
        self.cfg = cfg

        # 1. Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ (ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡)
        self.frontend = hydra.utils.instantiate(cfg.frontend)
        self.backbone = hydra.utils.instantiate(cfg.model)
        
        # Ð•ÑÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ„Ð¸Ð³Ðµ ÐµÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³ compile=True
        if cfg.get("compile", False):
            print("ðŸš€ EXTREME SPEEDUP: Compiling Backbone with torch.compile()...")
            # Windows Ð¸Ð¼ÐµÐµÑ‚ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½ÑƒÑŽ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ, Ð½Ð¾ 'default' Ð¸Ð»Ð¸ 'inductor' Ñ‡Ð°ÑÑ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚
            # ÐÐ° Linux Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾ mode="reduce-overhead"
            try:
                self.backbone = torch.compile(self.backbone)
                self.frontend = torch.compile(self.frontend)
                print("âœ… Compilation initialized.")
            except Exception as e:
                print(f"âš ï¸ Compilation failed (Windows?): {e}")  
        
        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°
        self.embed_dim = self.backbone.embed_dim if hasattr(self.backbone, 'embed_dim') else 2048

        # 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÐµÑÐ¾Ð² Ð¾Ñ‚ Ð´Ñ€ÑƒÐ³Ð¾Ð³Ð¾ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð° (Chaining)
        if cfg.get("pretrained_from"):
            self._load_pretrained_weights(cfg.pretrained_from)

    def _load_pretrained_weights(self, ckpt_path):
        print(f"ðŸ”„ Loading backbone weights from: {ckpt_path}")
        
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Pretrained checkpoint not found: {ckpt_path}")
            
        # Ð“Ñ€ÑƒÐ·Ð¸Ð¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ (Ð¾Ð±Ñ…Ð¾Ð´Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ñƒ PyTorch 2.6 Ð´Ð»Ñ Hydra)
        checkpoint = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state_dict = checkpoint["state_dict"]
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð²ÐµÑÐ°: Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ backbone
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("backbone."):
                # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€ÐµÑ„Ð¸ÐºÑ "backbone." Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¿Ñ€ÑÐ¼Ð¾ Ð² self.backbone
                new_key = k.replace("backbone.", "")
                new_state_dict[new_key] = v
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ (strict=False, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¼Ñ‹ Ð³Ñ€ÑƒÐ·Ð¸Ð¼ Ð¢ÐžÐ›Ð¬ÐšÐž Ð±ÑÐºÐ±Ð¾Ð½, Ð±ÐµÐ· Ð³Ð¾Ð»Ð¾Ð²Ñ‹)
        missing, unexpected = self.backbone.load_state_dict(new_state_dict, strict=False)
        print(f"âœ… Weights loaded. Missing: {len(missing)}, Unexpected: {len(unexpected)}")

    def configure_optimizers(self):
        # Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ‚Ð¾Ñ€ Ð¸Ð· Hydra
        optimizer = hydra.utils.instantiate(self.cfg.optimizer, params=self.parameters())
        if "scheduler" in self.cfg:
            scheduler = hydra.utils.instantiate(self.cfg.scheduler, optimizer=optimizer)
            return [optimizer], [scheduler]
        return optimizer

    def forward(self, x):
        # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´: ÐÑƒÐ´Ð¸Ð¾ -> Ð­Ð¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³
        return self.backbone(self.frontend(x))