import os
import glob
import pandas as pd
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from sklearn.preprocessing import LabelEncoder
from hydra.utils import to_absolute_path
import numpy as np

from src.data_loaders.bird_dataset import PrecomputedBirdDataset
from src.utils.serialization import load_pickle, save_pickle
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—ç–º–ø–ª–µ—Ä (—É–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª samplers.py –∏—Å–ø—Ä–∞–≤–ª–µ–Ω, –∫–∞–∫ –æ–±—Å—É–∂–¥–∞–ª–∏ —Ä–∞–Ω–µ–µ)
from src.data_loaders.samplers import MPerClassSampler

class BirdDataModule(pl.LightningDataModule):
    def __init__(self, root_dir, batch_size, num_workers, m_per_class=0, resize_shape=None, train_transform=None, val_transform=None ):
        super().__init__()
        self.root_dir = root_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.resize_shape = resize_shape
        self.train_transform = train_transform
        self.val_transform = val_transform 
        # –ï—Å–ª–∏ m_per_class > 0, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å—ç–º–ø–ª–µ—Ä. 
        # –ï—Å–ª–∏ 0 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) - –æ–±—ã—á–Ω—ã–π shuffle.
        self.m_per_class = m_per_class 
        
        self.label_encoder = None

    def setup(self, stage=None):
        abs_root = to_absolute_path(self.root_dir) if not os.path.isabs(self.root_dir) else self.root_dir
        
        # 1. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º DataFrame'—ã, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∫–ª–∞—Å—Å—ã —É –Ω–∞—Å –†–ï–ê–õ–¨–ù–û –µ—Å—Ç—å
        train_split_dir = os.path.join(abs_root, 'splits', 'train')
        train_dfs = []
        train_files = glob.glob(os.path.join(train_split_dir, "*.csv"))
        
        if not train_files:
             # Fallback
             fallback = os.path.join(abs_root, 'splits', 'train.csv')
             if os.path.exists(fallback):
                 train_files = [fallback]
             else:
                 # –ï—Å–ª–∏ —ç—Ç–æ SSL (–≥–¥–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å splits), –ø–æ–ø—Ä–æ–±—É–µ–º manifest
                 manifest_path = os.path.join(abs_root, 'manifest.csv')
                 if os.path.exists(manifest_path):
                     print("‚ö†Ô∏è No train splits found. Using full manifest as train (SSL mode?).")
                     train_files = [manifest_path]
                 else:
                     raise FileNotFoundError(f"No train csv files found in {train_split_dir}")

        for f in train_files:
            train_dfs.append(pd.read_csv(f))
        
        self.train_df = pd.concat(train_dfs, ignore_index=True)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_split_dir = os.path.join(abs_root, 'splits', 'val')
        val_dfs = []
        val_files = glob.glob(os.path.join(val_split_dir, "*.csv"))
        
        # –ï—Å–ª–∏ –≤ –ø–∞–ø–∫–µ –ø—É—Å—Ç–æ, –ø—Ä–æ–±—É–µ–º val.csv
        if not val_files:
            fallback_val = os.path.join(abs_root, 'splits', 'val.csv')
            if os.path.exists(fallback_val):
                val_files = [fallback_val]

        if val_files:
            for f in val_files:
                val_dfs.append(pd.read_csv(f))
            self.val_df = pd.concat(val_dfs, ignore_index=True)
        else:
            self.val_df = pd.DataFrame(columns=self.train_df.columns)

        test_split_dir = os.path.join(abs_root, 'splits', 'test')
        test_dfs = []
        test_files = glob.glob(os.path.join(test_split_dir, "*.csv"))
        
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ, –∏—â–µ–º test.csv
        if not test_files:
            fallback_test = os.path.join(abs_root, 'splits', 'test.csv')
            if os.path.exists(fallback_test):
                test_files = [fallback_test]

        if test_files:
            for f in test_files:
                test_dfs.append(pd.read_csv(f))
            self.test_df = pd.concat(test_dfs, ignore_index=True)
        else:
            self.test_df = pd.DataFrame(columns=self.train_df.columns)

        print(f"Dataset Loaded. Train: {len(self.train_df)}, Val: {len(self.val_df)}, Test: {len(self.test_df)}")

        # 2. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LabelEncoder –¢–û–õ–¨–ö–û –ø–æ Train (–∏ Val) –¥–∞–Ω–Ω—ã–º
        le_path = os.path.join(abs_root, 'checkpoints', 'label_encoder.pkl')
        
        # –õ–æ–≥–∏–∫–∞: –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢–ï–ö–£–©–ò–• –¥–∞–Ω–Ω—ã—Ö.
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å, –Ω–æ –º—ã —Ö–æ—Ç–∏–º –ø–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å - —É–¥–∞–ª–∏ –µ–≥–æ –≤—Ä—É—á–Ω—É—é –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º.
        if os.path.exists(le_path):
            print(f"Loading LabelEncoder from {le_path}")
            self.label_encoder = load_pickle(le_path)
        else:
            print("Creating LabelEncoder from TRAIN/VAL splits...")
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∏–¥—ã –∏–∑ —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            # Unseen test —Å—é–¥–∞ –ù–ï –ø–æ–ø–∞–¥–∞–µ—Ç!
            all_species = pd.concat([self.train_df['species'], self.val_df['species']]).dropna().unique()
            
            self.label_encoder = LabelEncoder()
            self.label_encoder.fit(all_species)
            
            save_pickle(self.label_encoder, le_path)
            print(f"LabelEncoder saved. Classes: {len(self.label_encoder.classes_)}")

        self.num_classes = len(self.label_encoder.classes_)

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        self.train_ds = PrecomputedBirdDataset(
            self.train_df, 
            self.label_encoder, 
            data_root=abs_root,
            resize_shape=self.resize_shape,
            augmentation=self.train_transform 
        )
        
        # –í–ê–õ–ò–î–ê–¶–ò–Ø
        self.val_ds = PrecomputedBirdDataset(
            self.val_df, 
            self.label_encoder, 
            data_root=abs_root,
            resize_shape=self.resize_shape,
            augmentation=self.val_transform # <--- –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º
        )
        
        # –¢–ï–°–¢
        self.test_ds = PrecomputedBirdDataset(
            self.test_df, 
            self.label_encoder, 
            data_root=abs_root,
            resize_shape=self.resize_shape,
            augmentation=self.val_transform # <--- –¢–æ–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ–º val_transform
        )

    def train_dataloader(self):
        # –ï—Å–ª–∏ m_per_class > 0 (Triplet mode), –∏—Å–ø–æ–ª—å–∑—É–µ–º Sampler
        if self.m_per_class > 0:
            print(f"Using MPerClassSampler (m={self.m_per_class})")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —Å—ç–º–ø–ª–µ—Ä–∞
            # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º transform, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —á–∏—Å–ª–∞
            # –ï—Å–ª–∏ –ø–æ–ø–∞–¥–µ—Ç—Å—è –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ (unseen), transform —É–ø–∞–¥–µ—Ç.
            # –≠—Ç–æ —Ö–æ—Ä–æ—à–æ! –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, –º—ã –Ω–∞—à–ª–∏ —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö.
            try:
                labels = self.label_encoder.transform(self.train_ds.df['species'].values)
            except ValueError as e:
                print("üî¥ CRITICAL ERROR: Found classes in Train that are not in LabelEncoder!")
                print("Try deleting checkpoints/label_encoder.pkl and restarting.")
                raise e
            
            sampler = MPerClassSampler(
                labels=labels, 
                m_per_class=self.m_per_class, 
                batch_size=self.batch_size
            )
            
            return DataLoader(
                self.train_ds, 
                batch_sampler=sampler, 
                num_workers=self.num_workers,
                pin_memory=True
            )
        else:
            # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º (Shuffle) –¥–ª—è CrossEntropy
            return DataLoader(
                self.train_ds, 
                batch_size=self.batch_size, 
                num_workers=self.num_workers, 
                shuffle=True, 
                pin_memory=True
            )

    def val_dataloader(self):
        if len(self.val_ds) == 0:
            return None
        return DataLoader(
            self.val_ds, 
            batch_size=self.batch_size, 
            num_workers=self.num_workers, 
            shuffle=False,
            pin_memory=True
        )
        
    def test_dataloader(self):
        if len(self.test_ds) == 0:
            return None
        return DataLoader(
            self.test_ds, 
            batch_size=self.batch_size, 
            num_workers=self.num_workers, 
            shuffle=False,
            pin_memory=True
        )