import hydra
from omegaconf import DictConfig, OmegaConf
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger
from hydra.utils import instantiate, get_class
import os
import torch

@hydra.main(config_path="configs", config_name="config", version_base="1.3")
def main(cfg: DictConfig):
    # 1. Resolve & Seed
    OmegaConf.resolve(cfg)
    pl.seed_everything(cfg.seed)
    
    # 2. Output Dir
    output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir
    print(f"üöÄ Experiment: {cfg.experiment_name} / {cfg.run_name}")
    print(f"üìÇ Output Dir: {output_dir}")

    # 3. –î–∞–Ω–Ω—ã–µ (DataModule)
    # --- –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ô ---
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–∏—à–ª–æ –≤ augmentation: —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ –∏–ª–∏ –∑–∞–≥–ª—É—à–∫–∞ (target=None)
    aug_config = cfg.get("augmentation")
    
    if aug_config is None or aug_config.get("_target_") is None:
        # –°–ª—É—á–∞–π "none.yaml" -> –í—ã–∫–ª—é—á–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –≤ –¥–∞–Ω–Ω—ã—Ö
        print("‚ö†Ô∏è  Augmentation: Disabled (None)")
        
        # –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç—Ä–µ–±—É–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –ø–∞–¥–∞–µ–º —Å –ø–æ–Ω—è—Ç–Ω–æ–π –æ—à–∏–±–∫–æ–π
        if cfg.task.get("name") in ["ssl_contrastive", "supcon_learning"]:
             raise ValueError(
                 f"‚ùå CRITICAL: Task '{cfg.task.name}' requires augmentation!\n"
                 "Please add `- override /augmentation: contrastive_strong` to your experiment file."
             )
        
        # –ó–∞–Ω—É–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º –≤ –∫–æ–Ω—Ñ–∏–≥–µ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        if "train_transform" in cfg.data:
            cfg.data.train_transform = None
            
    else:
        print(f"üîß Augmentation: {aug_config._target_}")
        # –ó–¥–µ—Å—å cfg.data.train_transform —É–∂–µ —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ cfg.augmentation
        # Hydra –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ—Ç —ç—Ç–æ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ.

    dm = instantiate(cfg.data)
    dm.setup()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º num_classes
    if hasattr(dm, "num_classes") and dm.num_classes:
        print(f"‚ÑπÔ∏è  Detected {dm.num_classes} classes.")
        cfg.model.num_classes = dm.num_classes
    else:
        cfg.model.num_classes = 0

    # 4. –°–∏—Å—Ç–µ–º–∞ (–ú–æ–¥–µ–ª—å)
    print(f"üß† System Class: {cfg.task.system_class._target_}")
    SystemClass = get_class(cfg.task.system_class._target_)
    model = SystemClass(cfg) 

    # 5. Logger & Callbacks
    logger = CSVLogger(save_dir=output_dir, name="logs", version="")
    ckpt_cb = instantiate(cfg.callbacks.model_checkpoint, dirpath=os.path.join(output_dir, "checkpoints"))
    reporter = instantiate(cfg.callbacks.reporter, cfg=cfg, output_dir=output_dir)

    # 6. Trainer
    trainer = instantiate(
        cfg.trainer,
        callbacks=[ckpt_cb, reporter],
        logger=logger
    )

    # 7. Train
    trainer.fit(model, dm)
    
    # 8. Test (Only for supervised tasks)
    if cfg.task.get("name") in ["classification", "metric_learning"]:
        print("\nüß™ Starting Test Cycle...")
        if ckpt_cb.best_model_path and os.path.exists(ckpt_cb.best_model_path):
            print(f"Loading best: {ckpt_cb.best_model_path}")
            checkpoint = torch.load(ckpt_cb.best_model_path, map_location="cpu", weights_only=False)
            model.load_state_dict(checkpoint["state_dict"])
            trainer.test(model, datamodule=dm)
        else:
            print("‚ö†Ô∏è Testing with final weights (no checkpoint found).")
            trainer.test(model, datamodule=dm)

if __name__ == "__main__":
    main()